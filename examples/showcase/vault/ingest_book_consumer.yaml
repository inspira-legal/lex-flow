# Vault - PubSub consumer for document ingestion (pgVector + API integration)
#
# Long-running process that subscribes to the document-uploaded PubSub topic
# and processes each document message through the RAG pipeline.
#
# Message format (from API's load-documents script):
#   {
#     "type": "document-uploaded",
#     "version": 1,
#     "data": {
#       "documentId": "mongo-object-id",
#       "workspaceId": "workspace-mongo-id",
#       "gcsUri": "gs://bucket-name/path/to/file.pdf",
#       "fileName": "document.pdf",
#       "mimeType": "application/pdf"
#     }
#   }
#
# For each message:
#   1. Callback: status -> PROCESSING
#   2. Download PDF from GCS (parsed from gcsUri)
#   3. Extract text from PDF
#   4. Chunk text with overlap
#   5. Generate embeddings (Vertex AI)
#   6. Build payloads (with document_id, workspace_id, file_name in metadata)
#   7. Upsert to pgVector
#   8. Callback: status -> INDEXED (with totalChunks + totalPages)
#   9. ACK message
#   On error: Callback status -> ERROR, NACK message for retry
#
# Usage:
#   uv run lexflow examples/showcase/vault/ingest_book_consumer.yaml \
#     --input api_token=YOUR_JWT_TOKEN
#
# Optional:
#   --input project_id=inspira-development
#   --input subscription_id=document-uploaded-sub
#   --input pgvector_dsn=postgresql://vault:vault@localhost/vault
#   --input collection=documents
#   --input api_url=http://localhost:4000/graphql
#
# Requirements:
#   pip install lexflow[pgvector,gcs,pubsub]
#   gcloud auth application-default login
#   PostgreSQL with pgvector extension running
#   PubSub subscription created

workflows:
  - name: main
    interface:
      inputs: [api_token, project_id, subscription_id, pgvector_dsn, collection, api_url, embedding_model, chunk_size, overlap]
      outputs: []
    variables:
      # Configuration
      api_token: ""
      project_id: "inspira-development"
      subscription_id: "document-uploaded-sub"
      pgvector_dsn: "postgresql://vault:vault@localhost/vault"
      collection: "documents"
      api_url: "http://localhost:4000/graphql"
      embedding_model: "text-multilingual-embedding-002"
      chunk_size: 1000
      overlap: 200
      # Clients (created once, reused)
      subscriber: null
      gcs_client: null
      pgvector_pool: null
      # Per-message state
      msg: null
      inner_data: null
      document_id: ""
      workspace_id: ""
      gcs_uri: ""
      file_name: ""
      bucket: ""
      object_path: ""
      source_path: ""
      pdf_bytes: null
      pdf_pages: []
      page_count: 0
      chunks: []
      chunk_count: 0
      embeddings: []
      ids: []
      payloads: []
      metadata: {}
      build_result: {}
      callback_body: {}
      total_processed: 0
      error_msg: ""
      # Timing
      step_t0: 0
      step_t1: 0
      msg_t0: 0
    nodes:
      start:
        opcode: workflow_start
        next: print_header
        inputs: {}

      # ======================================================================
      # HEADER
      # ======================================================================
      print_header:
        opcode: io_print
        next: print_title
        inputs:
          STRING:
            literal: "\n================================================================================\n"

      print_title:
        opcode: io_print
        next: print_separator
        inputs:
          STRING:
            literal: "          VAULT - DOCUMENT INGESTION CONSUMER (pgVector + API)\n"

      print_separator:
        opcode: io_print
        next: print_sub_info
        inputs:
          STRING:
            literal: "================================================================================\n\n"

      format_sub_info:
        opcode: string_format
        isReporter: true
        inputs:
          template:
            literal: "Subscription: {0}\npgVector:     {1}\nCollection:   {2}\nAPI:          {3}\n\n"
          VALUES:
            variable: subscription_id
          VALUE_1:
            variable: pgvector_dsn
          VALUE_2:
            variable: collection
          VALUE_3:
            variable: api_url

      print_sub_info:
        opcode: io_print
        next: init_clients
        inputs:
          STRING:
            node: format_sub_info

      # ======================================================================
      # CREATE CLIENTS (parallel via control_fork)
      # ======================================================================
      init_clients:
        opcode: control_fork
        next: print_init_done
        inputs:
          BRANCH1:
            branch: init_subscriber_branch
          BRANCH2:
            branch: init_gcs_branch
          BRANCH3:
            branch: init_pgvector_branch

      # --- Subscriber branch ---
      create_subscriber:
        opcode: pubsub_create_subscriber
        isReporter: true
        inputs: {}

      init_subscriber_branch:
        opcode: data_set_variable_to
        next: print_subscriber_ok
        inputs:
          VARIABLE:
            literal: "subscriber"
          VALUE:
            node: create_subscriber

      print_subscriber_ok:
        opcode: io_print
        next: null
        inputs:
          STRING:
            literal: "[init] Subscriber criado\n"

      # --- GCS branch ---
      do_create_gcs_client:
        opcode: gcs_create_client
        isReporter: true
        inputs: {}

      init_gcs_branch:
        opcode: data_set_variable_to
        next: print_gcs_ok
        inputs:
          VARIABLE:
            literal: "gcs_client"
          VALUE:
            node: do_create_gcs_client

      print_gcs_ok:
        opcode: io_print
        next: null
        inputs:
          STRING:
            literal: "[init] Cliente GCS criado\n"

      # --- pgVector branch ---
      do_connect_pgvector:
        opcode: pgvector_connect
        isReporter: true
        inputs:
          dsn:
            variable: pgvector_dsn

      init_pgvector_branch:
        opcode: data_set_variable_to
        next: print_pgvector_ok
        inputs:
          VARIABLE:
            literal: "pgvector_pool"
          VALUE:
            node: do_connect_pgvector

      print_pgvector_ok:
        opcode: io_print
        next: null
        inputs:
          STRING:
            literal: "[init] pgVector conectado\n"

      print_init_done:
        opcode: io_print
        next: ensure_collection
        inputs:
          STRING:
            literal: "[init] Todos os clientes prontos\n"

      # Ensure collection exists
      do_create_collection:
        opcode: pgvector_create_collection
        isReporter: true
        inputs:
          pool:
            variable: pgvector_pool
          name:
            variable: collection
          vector_size:
            literal: 768

      ensure_collection:
        opcode: control_if_else
        next: print_waiting
        inputs:
          CONDITION:
            node: do_create_collection
          THEN:
            branch: print_collection_created
          ELSE:
            branch: print_collection_exists

      print_collection_created:
        opcode: io_print
        next: null
        inputs:
          STRING:
            literal: "[init] Collection criada\n"

      print_collection_exists:
        opcode: io_print
        next: null
        inputs:
          STRING:
            literal: "[init] Collection ja existe\n"

      print_waiting:
        opcode: io_print
        next: process_messages
        inputs:
          STRING:
            literal: "\nAguardando mensagens...\n\n"

      # ======================================================================
      # SUBSCRIBE & PROCESS LOOP
      # ======================================================================
      subscribe_stream:
        opcode: pubsub_subscribe_stream
        isReporter: true
        inputs:
          subscriber:
            variable: subscriber
          project_id:
            variable: project_id
          subscription_id:
            variable: subscription_id
          timeout:
            literal: null
          max_messages:
            literal: null
          batch_size:
            literal: 10
          min_poll_interval:
            literal: 0.1
          max_poll_interval:
            literal: 5.0

      process_messages:
        opcode: control_async_foreach
        next: done
        inputs:
          VAR:
            literal: "msg"
          ITERABLE:
            node: subscribe_stream
          BODY:
            branch: parse_message_data

      # ======================================================================
      # MESSAGE HANDLER: Parse API envelope and extract fields
      # ======================================================================
      # Step 1: Get raw data string from PubSub message
      get_msg_data_raw:
        opcode: dict_get
        isReporter: true
        inputs:
          d:
            variable: msg
          key:
            literal: "data"

      # Step 2: Parse JSON string into dict
      parse_msg_json:
        opcode: json_parse
        isReporter: true
        inputs:
          text:
            node: get_msg_data_raw

      # Step 3: Get inner "data" from API envelope { type, version, data: {...} }
      get_inner_data:
        opcode: dict_get
        isReporter: true
        inputs:
          d:
            node: parse_msg_json
          key:
            literal: "data"

      parse_message_data:
        opcode: data_set_variable_to
        next: save_document_id
        inputs:
          VARIABLE:
            literal: "inner_data"
          VALUE:
            node: get_inner_data

      # ======================================================================
      # EXTRACT FIELDS (camelCase from API envelope)
      # ======================================================================
      extract_document_id:
        opcode: dict_get
        isReporter: true
        inputs:
          d:
            variable: inner_data
          key:
            literal: "documentId"

      save_document_id:
        opcode: data_set_variable_to
        next: save_workspace_id
        inputs:
          VARIABLE:
            literal: "document_id"
          VALUE:
            node: extract_document_id

      extract_workspace_id:
        opcode: dict_get
        isReporter: true
        inputs:
          d:
            variable: inner_data
          key:
            literal: "workspaceId"

      save_workspace_id:
        opcode: data_set_variable_to
        next: save_gcs_uri
        inputs:
          VARIABLE:
            literal: "workspace_id"
          VALUE:
            node: extract_workspace_id

      extract_gcs_uri:
        opcode: dict_get
        isReporter: true
        inputs:
          d:
            variable: inner_data
          key:
            literal: "gcsUri"

      save_gcs_uri:
        opcode: data_set_variable_to
        next: save_file_name
        inputs:
          VARIABLE:
            literal: "gcs_uri"
          VALUE:
            node: extract_gcs_uri

      extract_file_name:
        opcode: dict_get
        isReporter: true
        inputs:
          d:
            variable: inner_data
          key:
            literal: "fileName"

      save_file_name:
        opcode: data_set_variable_to
        next: parse_gcs_uri
        inputs:
          VARIABLE:
            literal: "file_name"
          VALUE:
            node: extract_file_name

      # ======================================================================
      # PARSE GCS URI: gs://bucket/path/to/file -> bucket, path/to/file
      # ======================================================================
      # Split "gs://bucket/path/to/file" by "/"
      # Result: ["gs:", "", "bucket", "path", "to", "file"]
      # Index 2 = bucket, Index 3+ = object path
      split_gcs_uri:
        opcode: string_split
        isReporter: true
        inputs:
          text:
            variable: gcs_uri
          delimiter:
            literal: "/"

      # Get bucket name (index 2)
      get_bucket_from_uri:
        opcode: list_get
        isReporter: true
        inputs:
          items:
            node: split_gcs_uri
          index:
            literal: 2

      parse_gcs_uri:
        opcode: data_set_variable_to
        next: build_object_path
        inputs:
          VARIABLE:
            literal: "bucket"
          VALUE:
            node: get_bucket_from_uri

      # Build object path: remove "gs://bucket/" prefix
      # Start position = 5 (gs://) + bucket_length + 1 (/)
      get_bucket_length:
        opcode: string_length
        isReporter: true
        inputs:
          text:
            variable: bucket

      calc_prefix_length_part1:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            literal: 5
          right:
            node: get_bucket_length

      calc_prefix_length:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            node: calc_prefix_length_part1
          right:
            literal: 1

      extract_object_path:
        opcode: string_substring
        isReporter: true
        inputs:
          text:
            variable: gcs_uri
          start:
            node: calc_prefix_length

      build_object_path:
        opcode: data_set_variable_to
        next: build_source_path
        inputs:
          VARIABLE:
            literal: "object_path"
          VALUE:
            node: extract_object_path

      # Build display source path: gs://bucket/object_path
      source_path_prefix:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            literal: "gs://"
          right:
            variable: bucket

      source_path_suffix:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            literal: "/"
          right:
            variable: object_path

      source_path_full:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            node: source_path_prefix
          right:
            node: source_path_suffix

      build_source_path:
        opcode: data_set_variable_to
        next: save_msg_t0
        inputs:
          VARIABLE:
            literal: "source_path"
          VALUE:
            node: source_path_full

      # Save message start time
      get_msg_t0:
        opcode: util_time_now
        isReporter: true
        inputs: {}

      save_msg_t0:
        opcode: data_set_variable_to
        next: print_processing
        inputs:
          VARIABLE:
            literal: "msg_t0"
          VALUE:
            node: get_msg_t0

      # Print processing message
      format_processing_msg:
        opcode: string_format
        isReporter: true
        inputs:
          template:
            literal: "[{0}] Processando {1} ({2})...\n"
          VALUES:
            variable: document_id
          VALUE_1:
            variable: source_path
          VALUE_2:
            variable: file_name

      print_processing:
        opcode: io_print
        next: try_ingest
        inputs:
          STRING:
            node: format_processing_msg

      # ======================================================================
      # TRY/CATCH per message
      # ======================================================================
      try_ingest:
        opcode: control_try
        next: null
        inputs:
          TRY:
            branch: try_callback_processing
          CATCH1:
            exception_type: "Exception"
            var: "error_msg"
            body:
              branch: handle_error

      # ======================================================================
      # AUTH HEADERS (Bearer token, reused by all callbacks)
      # ======================================================================
      clean_api_token_newline:
        opcode: string_replace
        isReporter: true
        inputs:
          text:
            variable: api_token
          old:
            literal: "\n"
          new:
            literal: ""

      clean_api_token:
        opcode: string_replace
        isReporter: true
        inputs:
          text:
            node: clean_api_token_newline
          old:
            literal: "\r"
          new:
            literal: ""

      build_auth_header:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            literal: "Bearer "
          right:
            node: clean_api_token

      callback_headers_base:
        opcode: dict_create
        isReporter: true
        inputs: {}

      callback_headers_with_auth:
        opcode: dict_set
        isReporter: true
        inputs:
          d:
            node: callback_headers_base
          key:
            literal: "Authorization"
          value:
            node: build_auth_header

      callback_headers:
        opcode: dict_set
        isReporter: true
        inputs:
          d:
            node: callback_headers_with_auth
          key:
            literal: "Content-Type"
          value:
            literal: "application/json"

      # ======================================================================
      # STEP 1: Callback PROCESSING
      # ======================================================================
      build_processing_query:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            literal: 'mutation { updateDocumentStatus(input: { documentId: "'
          right:
            node: pq_2

      pq_2:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            variable: document_id
          right:
            literal: '", status: PROCESSING }) { id status } }'

      empty_dict_processing:
        opcode: dict_create
        isReporter: true
        inputs: {}

      build_processing_body:
        opcode: dict_set
        isReporter: true
        inputs:
          d:
            node: empty_dict_processing
          key:
            literal: "query"
          value:
            node: build_processing_query

      do_callback_processing:
        opcode: http_post
        isReporter: true
        inputs:
          url:
            variable: api_url
          data:
            literal: null
          json:
            node: build_processing_body
          headers:
            node: callback_headers

      try_callback_processing:
        opcode: control_try
        next: step_download_start
        inputs:
          TRY:
            branch: trigger_callback_processing
          CATCH1:
            exception_type: "Exception"
            var: "error_msg"
            body:
              branch: warn_callback_processing

      trigger_callback_processing:
        opcode: data_set_variable_to
        next: print_callback_processing
        inputs:
          VARIABLE:
            literal: "callback_body"
          VALUE:
            node: do_callback_processing

      print_callback_processing:
        opcode: io_print
        next: null
        inputs:
          STRING:
            literal: "  [1/8] Callback: PROCESSING\n"

      warn_callback_processing:
        opcode: io_print
        next: null
        inputs:
          STRING:
            literal: "  [1/8] Callback: PROCESSING (skipped - API unavailable)\n"

      # ======================================================================
      # STEP 2: Download PDF
      # ======================================================================
      get_step_t0_download:
        opcode: util_time_now
        isReporter: true
        inputs: {}

      step_download_start:
        opcode: data_set_variable_to
        next: download_pdf
        inputs:
          VARIABLE:
            literal: "step_t0"
          VALUE:
            node: get_step_t0_download

      do_download_pdf:
        opcode: gcs_download_object_as_bytes
        isReporter: true
        inputs:
          client:
            variable: gcs_client
          bucket_name:
            variable: bucket
          object_name:
            variable: object_path

      download_pdf:
        opcode: data_set_variable_to
        next: save_step_t1_download
        inputs:
          VARIABLE:
            literal: "pdf_bytes"
          VALUE:
            node: do_download_pdf

      get_step_t1_download:
        opcode: util_time_now
        isReporter: true
        inputs: {}

      save_step_t1_download:
        opcode: data_set_variable_to
        next: print_download_done
        inputs:
          VARIABLE:
            literal: "step_t1"
          VALUE:
            node: get_step_t1_download

      dur_download:
        opcode: util_format_duration
        isReporter: true
        inputs:
          start:
            variable: step_t0
          end:
            variable: step_t1

      fmt_download:
        opcode: string_format
        isReporter: true
        inputs:
          template:
            literal: "  [2/8] Download PDF:        OK ({0})\n"
          VALUES:
            node: dur_download

      print_download_done:
        opcode: io_print
        next: step_extract_start
        inputs:
          STRING:
            node: fmt_download

      # ======================================================================
      # STEP 3: Extract pages
      # ======================================================================
      get_step_t0_extract:
        opcode: util_time_now
        isReporter: true
        inputs: {}

      step_extract_start:
        opcode: data_set_variable_to
        next: extract_pdf
        inputs:
          VARIABLE:
            literal: "step_t0"
          VALUE:
            node: get_step_t0_extract

      do_extract_pdf:
        opcode: pdf_extract_pages_from_bytes
        isReporter: true
        inputs:
          data:
            variable: pdf_bytes

      extract_pdf:
        opcode: data_set_variable_to
        next: store_page_count
        inputs:
          VARIABLE:
            literal: "pdf_pages"
          VALUE:
            node: do_extract_pdf

      get_page_count:
        opcode: list_length
        isReporter: true
        inputs:
          items:
            variable: pdf_pages

      store_page_count:
        opcode: data_set_variable_to
        next: save_step_t1_extract
        inputs:
          VARIABLE:
            literal: "page_count"
          VALUE:
            node: get_page_count

      get_step_t1_extract:
        opcode: util_time_now
        isReporter: true
        inputs: {}

      save_step_t1_extract:
        opcode: data_set_variable_to
        next: print_extract_done
        inputs:
          VARIABLE:
            literal: "step_t1"
          VALUE:
            node: get_step_t1_extract

      dur_extract:
        opcode: util_format_duration
        isReporter: true
        inputs:
          start:
            variable: step_t0
          end:
            variable: step_t1

      fmt_extract:
        opcode: string_format
        isReporter: true
        inputs:
          template:
            literal: "  [3/8] Extract pages:       OK ({0})\n"
          VALUES:
            node: dur_extract

      print_extract_done:
        opcode: io_print
        next: step_chunk_start
        inputs:
          STRING:
            node: fmt_extract

      # ======================================================================
      # STEP 4: Chunk text
      # ======================================================================
      get_step_t0_chunk:
        opcode: util_time_now
        isReporter: true
        inputs: {}

      step_chunk_start:
        opcode: data_set_variable_to
        next: chunk_text
        inputs:
          VARIABLE:
            literal: "step_t0"
          VALUE:
            node: get_step_t0_chunk

      do_chunk_pages:
        opcode: text_chunk_pages_smart
        isReporter: true
        inputs:
          pages:
            variable: pdf_pages
          chunk_size:
            variable: chunk_size
          overlap:
            variable: overlap

      chunk_text:
        opcode: data_set_variable_to
        next: store_chunk_count
        inputs:
          VARIABLE:
            literal: "chunks"
          VALUE:
            node: do_chunk_pages

      get_chunk_count:
        opcode: list_length
        isReporter: true
        inputs:
          items:
            variable: chunks

      store_chunk_count:
        opcode: data_set_variable_to
        next: save_step_t1_chunk
        inputs:
          VARIABLE:
            literal: "chunk_count"
          VALUE:
            node: get_chunk_count

      get_step_t1_chunk:
        opcode: util_time_now
        isReporter: true
        inputs: {}

      save_step_t1_chunk:
        opcode: data_set_variable_to
        next: print_chunk_done
        inputs:
          VARIABLE:
            literal: "step_t1"
          VALUE:
            node: get_step_t1_chunk

      dur_chunk:
        opcode: util_format_duration
        isReporter: true
        inputs:
          start:
            variable: step_t0
          end:
            variable: step_t1

      fmt_chunk:
        opcode: string_format
        isReporter: true
        inputs:
          template:
            literal: "  [4/8] Chunk text:          OK ({0})\n"
          VALUES:
            node: dur_chunk

      print_chunk_done:
        opcode: io_print
        next: step_embed_start
        inputs:
          STRING:
            node: fmt_chunk

      # ======================================================================
      # STEP 5: Generate embeddings
      # ======================================================================
      get_step_t0_embed:
        opcode: util_time_now
        isReporter: true
        inputs: {}

      step_embed_start:
        opcode: data_set_variable_to
        next: generate_embeddings
        inputs:
          VARIABLE:
            literal: "step_t0"
          VALUE:
            node: get_step_t0_embed

      do_extract_chunk_texts:
        opcode: list_pluck
        isReporter: true
        inputs:
          items:
            variable: chunks
          key:
            literal: "text"

      do_generate_embeddings:
        opcode: embedding_create_batch
        isReporter: true
        inputs:
          texts:
            node: do_extract_chunk_texts
          project:
            variable: project_id
          location:
            literal: "us-central1"
          model:
            variable: embedding_model
          task_type:
            literal: "RETRIEVAL_DOCUMENT"

      generate_embeddings:
        opcode: data_set_variable_to
        next: save_step_t1_embed
        inputs:
          VARIABLE:
            literal: "embeddings"
          VALUE:
            node: do_generate_embeddings

      get_step_t1_embed:
        opcode: util_time_now
        isReporter: true
        inputs: {}

      save_step_t1_embed:
        opcode: data_set_variable_to
        next: print_embed_done
        inputs:
          VARIABLE:
            literal: "step_t1"
          VALUE:
            node: get_step_t1_embed

      dur_embed:
        opcode: util_format_duration
        isReporter: true
        inputs:
          start:
            variable: step_t0
          end:
            variable: step_t1

      fmt_embed:
        opcode: string_format
        isReporter: true
        inputs:
          template:
            literal: "  [5/8] Generate embeddings: OK ({0})\n"
          VALUES:
            node: dur_embed

      print_embed_done:
        opcode: io_print
        next: step_build_start
        inputs:
          STRING:
            node: fmt_embed

      # ======================================================================
      # STEP 6: Build payloads (with document_id, workspace_id, file_name)
      # ======================================================================
      get_step_t0_build:
        opcode: util_time_now
        isReporter: true
        inputs: {}

      step_build_start:
        opcode: data_set_variable_to
        next: build_metadata
        inputs:
          VARIABLE:
            literal: "step_t0"
          VALUE:
            node: get_step_t0_build

      # Build metadata dict with API fields
      metadata_base:
        opcode: dict_create
        isReporter: true
        inputs: {}

      metadata_with_source:
        opcode: dict_set
        isReporter: true
        inputs:
          d:
            node: metadata_base
          key:
            literal: "source"
          value:
            variable: source_path

      metadata_with_document_id:
        opcode: dict_set
        isReporter: true
        inputs:
          d:
            node: metadata_with_source
          key:
            literal: "document_id"
          value:
            variable: document_id

      metadata_with_workspace_id:
        opcode: dict_set
        isReporter: true
        inputs:
          d:
            node: metadata_with_document_id
          key:
            literal: "workspace_id"
          value:
            variable: workspace_id

      metadata_with_file_name:
        opcode: dict_set
        isReporter: true
        inputs:
          d:
            node: metadata_with_workspace_id
          key:
            literal: "file_name"
          value:
            variable: file_name

      build_metadata:
        opcode: data_set_variable_to
        next: do_build_payloads
        inputs:
          VARIABLE:
            literal: "metadata"
          VALUE:
            node: metadata_with_file_name

      # Call rag_build_chunk_payloads
      do_rag_build:
        opcode: rag_build_chunk_payloads
        isReporter: true
        inputs:
          chunks:
            variable: chunks
          metadata:
            variable: metadata

      do_build_payloads:
        opcode: data_set_variable_to
        next: extract_ids
        inputs:
          VARIABLE:
            literal: "build_result"
          VALUE:
            node: do_rag_build

      # Extract ids and payloads from result
      get_built_ids:
        opcode: dict_get
        isReporter: true
        inputs:
          d:
            variable: build_result
          key:
            literal: "ids"

      extract_ids:
        opcode: data_set_variable_to
        next: extract_payloads
        inputs:
          VARIABLE:
            literal: "ids"
          VALUE:
            node: get_built_ids

      get_built_payloads:
        opcode: dict_get
        isReporter: true
        inputs:
          d:
            variable: build_result
          key:
            literal: "payloads"

      extract_payloads:
        opcode: data_set_variable_to
        next: save_step_t1_build
        inputs:
          VARIABLE:
            literal: "payloads"
          VALUE:
            node: get_built_payloads

      get_step_t1_build:
        opcode: util_time_now
        isReporter: true
        inputs: {}

      save_step_t1_build:
        opcode: data_set_variable_to
        next: print_build_done
        inputs:
          VARIABLE:
            literal: "step_t1"
          VALUE:
            node: get_step_t1_build

      dur_build:
        opcode: util_format_duration
        isReporter: true
        inputs:
          start:
            variable: step_t0
          end:
            variable: step_t1

      fmt_build:
        opcode: string_format
        isReporter: true
        inputs:
          template:
            literal: "  [6/8] Build payloads:      OK ({0})\n"
          VALUES:
            node: dur_build

      print_build_done:
        opcode: io_print
        next: step_upsert_start
        inputs:
          STRING:
            node: fmt_build

      # ======================================================================
      # STEP 7: Upsert to pgVector
      # ======================================================================
      get_step_t0_upsert:
        opcode: util_time_now
        isReporter: true
        inputs: {}

      step_upsert_start:
        opcode: data_set_variable_to
        next: do_upsert
        inputs:
          VARIABLE:
            literal: "step_t0"
          VALUE:
            node: get_step_t0_upsert

      do_upsert:
        opcode: pgvector_upsert_batch
        next: save_step_t1_upsert
        inputs:
          pool:
            variable: pgvector_pool
          collection:
            variable: collection
          point_ids:
            variable: ids
          vectors:
            variable: embeddings
          payloads:
            variable: payloads

      get_step_t1_upsert:
        opcode: util_time_now
        isReporter: true
        inputs: {}

      save_step_t1_upsert:
        opcode: data_set_variable_to
        next: print_upsert_done
        inputs:
          VARIABLE:
            literal: "step_t1"
          VALUE:
            node: get_step_t1_upsert

      dur_upsert:
        opcode: util_format_duration
        isReporter: true
        inputs:
          start:
            variable: step_t0
          end:
            variable: step_t1

      fmt_upsert:
        opcode: string_format
        isReporter: true
        inputs:
          template:
            literal: "  [7/8] Upsert pgVector:     OK ({0})\n"
          VALUES:
            node: dur_upsert

      print_upsert_done:
        opcode: io_print
        next: try_callback_indexed
        inputs:
          STRING:
            node: fmt_upsert

      # ======================================================================
      # STEP 8: Callback INDEXED (with totalChunks + totalPages)
      # ======================================================================
      chunk_count_str:
        opcode: str
        isReporter: true
        inputs:
          value:
            variable: chunk_count

      page_count_str:
        opcode: str
        isReporter: true
        inputs:
          value:
            variable: page_count

      build_indexed_query:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            literal: 'mutation { updateDocumentStatus(input: { documentId: "'
          right:
            node: iq_2

      iq_2:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            variable: document_id
          right:
            node: iq_3

      iq_3:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            literal: '", status: INDEXED, totalChunks: '
          right:
            node: iq_4

      iq_4:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            node: chunk_count_str
          right:
            node: iq_5

      iq_5:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            literal: ", totalPages: "
          right:
            node: iq_6

      iq_6:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            node: page_count_str
          right:
            literal: ' }) { id status } }'

      empty_dict_indexed:
        opcode: dict_create
        isReporter: true
        inputs: {}

      build_indexed_body:
        opcode: dict_set
        isReporter: true
        inputs:
          d:
            node: empty_dict_indexed
          key:
            literal: "query"
          value:
            node: build_indexed_query

      do_callback_indexed:
        opcode: http_post
        isReporter: true
        inputs:
          url:
            variable: api_url
          data:
            literal: null
          json:
            node: build_indexed_body
          headers:
            node: callback_headers

      try_callback_indexed:
        opcode: control_try
        next: ack_message
        inputs:
          TRY:
            branch: trigger_callback_indexed
          CATCH1:
            exception_type: "Exception"
            var: "error_msg"
            body:
              branch: warn_callback_indexed

      trigger_callback_indexed:
        opcode: data_set_variable_to
        next: print_callback_indexed
        inputs:
          VARIABLE:
            literal: "callback_body"
          VALUE:
            node: do_callback_indexed

      print_callback_indexed:
        opcode: io_print
        next: null
        inputs:
          STRING:
            literal: "  [8/8] Callback: INDEXED\n"

      warn_callback_indexed:
        opcode: io_print
        next: null
        inputs:
          STRING:
            literal: "  [8/8] Callback: INDEXED (skipped - API unavailable)\n"

      # ======================================================================
      # ACK message + timing summary + counter
      # ======================================================================
      ack_message:
        opcode: pubsub_ack_message
        next: increment_total
        inputs:
          subscriber:
            variable: subscriber
          project_id:
            variable: project_id
          subscription_id:
            variable: subscription_id
          message:
            variable: msg

      new_total:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            variable: total_processed
          right:
            literal: 1

      increment_total:
        opcode: data_set_variable_to
        next: print_total
        inputs:
          VARIABLE:
            literal: "total_processed"
          VALUE:
            node: new_total

      # Total message time
      get_msg_end:
        opcode: util_time_now
        isReporter: true
        inputs: {}

      dur_total:
        opcode: util_format_duration
        isReporter: true
        inputs:
          start:
            variable: msg_t0
          end:
            node: get_msg_end

      total_str:
        opcode: str
        isReporter: true
        inputs:
          value:
            variable: total_processed

      fmt_total:
        opcode: string_format
        isReporter: true
        inputs:
          template:
            literal: "  -- Total: {0} -- {1} chunks indexados (total processed: {2})\n\n"
          VALUES:
            node: dur_total
          VALUE_1:
            node: chunk_count_str
          VALUE_2:
            node: total_str

      print_total:
        opcode: io_print
        next: null
        inputs:
          STRING:
            node: fmt_total

      # ======================================================================
      # ERROR HANDLER: Callback ERROR + NACK message
      # ======================================================================
      handle_error:
        opcode: io_print
        next: print_error_detail
        inputs:
          STRING:
            literal: "  [ERROR] Processing failed: "

      format_error_detail:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            variable: error_msg
          right:
            literal: "\n"

      print_error_detail:
        opcode: io_print
        next: try_error_callback
        inputs:
          STRING:
            node: format_error_detail

      # Build error mutation
      build_error_mutation_1:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            literal: 'mutation { updateDocumentStatus(input: { documentId: "'
          right:
            variable: document_id

      build_error_mutation_2:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            node: build_error_mutation_1
          right:
            literal: '", status: ERROR, errorMessage: "'

      # Escape error message for GraphQL string
      escape_error_msg:
        opcode: string_replace
        isReporter: true
        inputs:
          text:
            variable: error_msg
          old:
            literal: "\""
          new:
            literal: "\\\""

      build_error_mutation_3:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            node: build_error_mutation_2
          right:
            node: escape_error_msg

      build_error_mutation:
        opcode: operator_add
        isReporter: true
        inputs:
          left:
            node: build_error_mutation_3
          right:
            literal: '" }) { id status } }'

      error_body_base:
        opcode: dict_create
        isReporter: true
        inputs: {}

      error_body:
        opcode: dict_set
        isReporter: true
        inputs:
          d:
            node: error_body_base
          key:
            literal: "query"
          value:
            node: build_error_mutation

      do_error_callback:
        opcode: http_post
        isReporter: true
        inputs:
          url:
            variable: api_url
          data:
            literal: null
          json:
            node: error_body
          headers:
            node: callback_headers

      try_error_callback:
        opcode: control_try
        next: nack_message
        inputs:
          TRY:
            branch: trigger_error_callback
          CATCH1:
            exception_type: "Exception"
            var: "error_msg"
            body:
              branch: warn_error_callback

      trigger_error_callback:
        opcode: data_set_variable_to
        next: print_error_callback_sent
        inputs:
          VARIABLE:
            literal: "callback_body"
          VALUE:
            node: do_error_callback

      print_error_callback_sent:
        opcode: io_print
        next: null
        inputs:
          STRING:
            literal: "  [ERROR] Callback: ERROR status sent\n"

      warn_error_callback:
        opcode: io_print
        next: null
        inputs:
          STRING:
            literal: "  [ERROR] Callback: ERROR status (skipped - API unavailable)\n"

      # NACK message for retry
      nack_message:
        opcode: pubsub_nack_message
        next: print_nack_done
        inputs:
          subscriber:
            variable: subscriber
          project_id:
            variable: project_id
          subscription_id:
            variable: subscription_id
          message:
            variable: msg

      print_nack_done:
        opcode: io_print
        next: null
        inputs:
          STRING:
            literal: "  [NACK] Message returned to queue for retry\n\n"

      # ======================================================================
      # CLEANUP
      # ======================================================================
      done:
        opcode: pubsub_close_subscriber
        next: disconnect_pgvector
        inputs:
          subscriber:
            variable: subscriber

      do_disconnect_pgvector:
        opcode: pgvector_disconnect
        isReporter: true
        inputs:
          pool:
            variable: pgvector_pool

      disconnect_pgvector:
        opcode: data_set_variable_to
        next: close_gcs
        inputs:
          VARIABLE:
            literal: "pgvector_pool"
          VALUE:
            node: do_disconnect_pgvector

      do_close_gcs:
        opcode: gcs_close_client
        isReporter: true
        inputs:
          client:
            variable: gcs_client

      close_gcs:
        opcode: data_set_variable_to
        next: print_done
        inputs:
          VARIABLE:
            literal: "gcs_client"
          VALUE:
            node: do_close_gcs

      print_done:
        opcode: io_print
        next: null
        inputs:
          STRING:
            literal: "\nConsumer encerrado.\n"
